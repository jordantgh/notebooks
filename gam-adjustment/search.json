[
  {
    "objectID": "gam-adjustment.html",
    "href": "gam-adjustment.html",
    "title": "Normalising Data Using Generalised Additive Models",
    "section": "",
    "text": "Here I explore a way to normalise data by partialling out the effect of a potential confounding variable using generalised additive models (GAM). This is useful when we want to adjust for confounders but we don’t know in advance the functional form of their relationships with the dependent variable. Of course, the GAM modelling results straightforwardly provide us with adjusted coefficients for all the variables in the model, and for most practical purposes this is sufficient. However, it may be that we would like to graphically plot relationships between the dependent variable and the individual covariates, and do so in a way that isolates their effects from those of other variables. Additionally, we might want to use this adjusted quantity for some downstream inference task, such as sorting observations by adjusted values and characterising top and bottom groups. So here I use mgcv to fit a GAM to some simulated data and explore how we can carry out this kind of normalisation.\n\n\nIt’s worth noting that if plotting is the goal, the plot.gam function does provide some basic functionality for this. Let’s first simulate some data and fit a GAM to it, then we can see what tools are available.\nLet’s start with a relatively simple model:\n\\[\ny = b_0 + b_1 x + b_2 z^2 + \\epsilon\n\\]\nwith \\(x \\sim \\text{Bernoulli}(0.5)\\), \\(z \\sim \\mathcal{N}(2, 1)\\), and \\(\\epsilon \\sim \\mathcal{N}(0, 100)\\).\nThe form is somewhat arbitrary, except that we have a non-linear term in \\(z\\) to demonstrate some of the advantages of using a GAM over a GLM. I have also made the error term fairly large to keep the p-value from being too small for meaningful comparisons.\n\nlibrary(mgcv)\nlibrary(dplyr)\n\nn &lt;- 1000\nx &lt;- rbinom(n, 1, 0.5)\nz &lt;- rnorm(n, mean = 0, sd = 1)\n\nb0 &lt;- 10\nb1 &lt;- 10\nb2 &lt;- 20\n\ny &lt;- b0 + b1 * x + b2 * z^2 + rnorm(n, mean = 0, sd = 10)\ndf &lt;- data.frame(Y = y, X = x, Z = z)\ngam_model &lt;- gam(Y ~ X + s(Z), data = df, method = \"REML\")\n\nI mentioned that there are we can use the plot.gam function to visualise individual relationships. We can use the all.terms argument for this purpose.\n\n\nCode\nplot(gam_model, pages = 1, all.terms = TRUE, rug = FALSE, residuals = TRUE)\n\n\n\n\n\n\n\n\n\nThese plots are OK, although they use the base R plotting system and are not very customisable. They also do not show results on a familiar scale, which reduces their immediate interpretability.\n\n\n\nThe approach I want to pursue is to generate derived quantities that we can do whatever we like with. Once we have the partial predictions for each row, we can store them and treat them as data, plotting them or otherwise analysing them as we like.\n\npreds &lt;- predict(gam_model, type = \"terms\") |&gt;\n  as.data.frame() |&gt;\n  setNames(c(\"pred_x\", \"pred_z\"))\n\ndf &lt;- cbind(df, preds)\n\nOn its own, this is not very informative. The pp object contains the predictions for each row of the data, but they are not on a scale that is immediately interpretable. We need to add back the intercept and the effect of \\(Z\\) to get the “normalised” \\(Y\\).\n\n\n\n\n\nY\nX\nZ\npred_x\npred_z\n\n\n\n\n48.94352\n1\n-1.0221207\n9.85626\n2.552670\n\n\n27.44250\n0\n0.6785096\n0.00000\n-9.234057\n\n\n36.95893\n1\n0.2136389\n9.85626\n-18.738867\n\n\n96.25313\n1\n1.9939274\n9.85626\n62.564750\n\n\n36.72771\n0\n1.0673085\n0.00000\n3.438250\n\n\n59.32940\n1\n1.3419030\n9.85626\n15.672172\n\n\n\n\n\n\n# Get the predicted values at the mean of Z since the 'terms' effects are\n# relative to the mean effects (for smoothed terms), which are themselves\n# absorbed into the intercept\nz_mean_effect &lt;- predict(\n  gam_model, newdata = data.frame(X = 0, Z = mean(df$Z))\n)\n\n# for reasons unclear tp me, getting the intercept via coefs(gam_model) gives a\n# result that is off by +b2, so just calculating it directly\nintercept &lt;- predict(gam_model, newdata = data.frame(X = 0, Z = 0))\nz_mean_effect &lt;- z_mean_effect - intercept\n\n# Now calculate the \"normalised\" Y by subtracting the effect of Z\ndf$z_eff_pred_y &lt;- df$pred_z + z_mean_effect\ndf$y_adj_z &lt;- y - df$z_eff_pred_y\n\n# check everything so far is correct by making sure we can reconstruct approx\n# the true Y from the estimated components\ndf$x_eff_pred &lt;- df$pred_x\ndf$pred_y_fromterms &lt;- intercept + df$x_eff_pred + df$z_eff_pred_y\nplot(df$pred_y_fromterms, df$Y) # straight line, so component terms are correct\n\n\n\n\n\n\n\n#summary(gam_model)\n#summary(lm(y_adj_z ~ X, data = df))"
  },
  {
    "objectID": "gam-adjustment.html#built-in-capacity-for-partialling-out-in-mgcv",
    "href": "gam-adjustment.html#built-in-capacity-for-partialling-out-in-mgcv",
    "title": "Normalising Data Using Generalised Additive Models",
    "section": "",
    "text": "It’s worth noting that if plotting is the goal, the plot.gam function does provide some basic functionality for this. Let’s first simulate some data and fit a GAM to it, then we can see what tools are available.\nLet’s start with a relatively simple model:\n\\[\ny = b_0 + b_1 x + b_2 z^2 + \\epsilon\n\\]\nwith \\(x \\sim \\text{Bernoulli}(0.5)\\), \\(z \\sim \\mathcal{N}(2, 1)\\), and \\(\\epsilon \\sim \\mathcal{N}(0, 100)\\).\nThe form is somewhat arbitrary, except that we have a non-linear term in \\(z\\) to demonstrate some of the advantages of using a GAM over a GLM. I have also made the error term fairly large to keep the p-value from being too small for meaningful comparisons.\n\nlibrary(mgcv)\nlibrary(dplyr)\n\nn &lt;- 1000\nx &lt;- rbinom(n, 1, 0.5)\nz &lt;- rnorm(n, mean = 0, sd = 1)\n\nb0 &lt;- 10\nb1 &lt;- 10\nb2 &lt;- 20\n\ny &lt;- b0 + b1 * x + b2 * z^2 + rnorm(n, mean = 0, sd = 10)\ndf &lt;- data.frame(Y = y, X = x, Z = z)\ngam_model &lt;- gam(Y ~ X + s(Z), data = df, method = \"REML\")\n\nI mentioned that there are we can use the plot.gam function to visualise individual relationships. We can use the all.terms argument for this purpose.\n\n\nCode\nplot(gam_model, pages = 1, all.terms = TRUE, rug = FALSE, residuals = TRUE)\n\n\n\n\n\n\n\n\n\nThese plots are OK, although they use the base R plotting system and are not very customisable. They also do not show results on a familiar scale, which reduces their immediate interpretability."
  },
  {
    "objectID": "gam-adjustment.html#beyond-plot.gam",
    "href": "gam-adjustment.html#beyond-plot.gam",
    "title": "Normalising Data Using Generalised Additive Models",
    "section": "",
    "text": "The approach I want to pursue is to generate derived quantities that we can do whatever we like with. Once we have the partial predictions for each row, we can store them and treat them as data, plotting them or otherwise analysing them as we like.\n\npreds &lt;- predict(gam_model, type = \"terms\") |&gt;\n  as.data.frame() |&gt;\n  setNames(c(\"pred_x\", \"pred_z\"))\n\ndf &lt;- cbind(df, preds)\n\nOn its own, this is not very informative. The pp object contains the predictions for each row of the data, but they are not on a scale that is immediately interpretable. We need to add back the intercept and the effect of \\(Z\\) to get the “normalised” \\(Y\\).\n\n\n\n\n\nY\nX\nZ\npred_x\npred_z\n\n\n\n\n48.94352\n1\n-1.0221207\n9.85626\n2.552670\n\n\n27.44250\n0\n0.6785096\n0.00000\n-9.234057\n\n\n36.95893\n1\n0.2136389\n9.85626\n-18.738867\n\n\n96.25313\n1\n1.9939274\n9.85626\n62.564750\n\n\n36.72771\n0\n1.0673085\n0.00000\n3.438250\n\n\n59.32940\n1\n1.3419030\n9.85626\n15.672172\n\n\n\n\n\n\n# Get the predicted values at the mean of Z since the 'terms' effects are\n# relative to the mean effects (for smoothed terms), which are themselves\n# absorbed into the intercept\nz_mean_effect &lt;- predict(\n  gam_model, newdata = data.frame(X = 0, Z = mean(df$Z))\n)\n\n# for reasons unclear tp me, getting the intercept via coefs(gam_model) gives a\n# result that is off by +b2, so just calculating it directly\nintercept &lt;- predict(gam_model, newdata = data.frame(X = 0, Z = 0))\nz_mean_effect &lt;- z_mean_effect - intercept\n\n# Now calculate the \"normalised\" Y by subtracting the effect of Z\ndf$z_eff_pred_y &lt;- df$pred_z + z_mean_effect\ndf$y_adj_z &lt;- y - df$z_eff_pred_y\n\n# check everything so far is correct by making sure we can reconstruct approx\n# the true Y from the estimated components\ndf$x_eff_pred &lt;- df$pred_x\ndf$pred_y_fromterms &lt;- intercept + df$x_eff_pred + df$z_eff_pred_y\nplot(df$pred_y_fromterms, df$Y) # straight line, so component terms are correct\n\n\n\n\n\n\n\n#summary(gam_model)\n#summary(lm(y_adj_z ~ X, data = df))"
  }
]