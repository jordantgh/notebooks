[
  {
    "objectID": "quarto-prompts.html",
    "href": "quarto-prompts.html",
    "title": "Quarto prompts to python functions",
    "section": "",
    "text": "Applications built around the use of calls to various LLM provider APIs (e.g., OpenAI, Anthropic, Google) are increasingly popular. I have built such an application as part of my PhD to extract structured information from scientific literature. Prompting LLMs with natural language instructions in the context of a highly structured codebase presents some challenges. How should you store prompts, and what’s the best way to give them dynamic features?\nI have developed a workflow that suits me, and I’m sharing it here. I am not explaining how it works here – I plan to write a blog about it and will link back here once that’s done.\nThe basic recipe is:\n\nPrompts are stored in a centralised directory and written as quarto markdown files (like this document!) - this gives us access to nice syntax highlighting and formatting, suited both for editing prompts in a readable way, and for working with jinja based template rendering approaches (more on this later).\nEvery time a prompt is updated & changes saved, a script runs converting the prompt into a python function.\nThe module holding all “prompt functions” can be loaded in to scripts where prompts are deployed.\n\nCurrently my solution will only work on linux. Other requirements:\n\ndirenv - this allows us to make a project-specific monitoring hook using an .envrc file\ninotify-tools - we use this for monitoring the prompts folder\nat - this allows our hook script (which calls inotifywait) to run from .envrc without hanging up the terminal\njinja2 python package - used for rendering prompt templates\nquarto - for a nice editing experience with syntax highlighting goodies"
  },
  {
    "objectID": "quarto-prompts.html#intro",
    "href": "quarto-prompts.html#intro",
    "title": "Quarto prompts to python functions",
    "section": "",
    "text": "Applications built around the use of calls to various LLM provider APIs (e.g., OpenAI, Anthropic, Google) are increasingly popular. I have built such an application as part of my PhD to extract structured information from scientific literature. Prompting LLMs with natural language instructions in the context of a highly structured codebase presents some challenges. How should you store prompts, and what’s the best way to give them dynamic features?\nI have developed a workflow that suits me, and I’m sharing it here. I am not explaining how it works here – I plan to write a blog about it and will link back here once that’s done.\nThe basic recipe is:\n\nPrompts are stored in a centralised directory and written as quarto markdown files (like this document!) - this gives us access to nice syntax highlighting and formatting, suited both for editing prompts in a readable way, and for working with jinja based template rendering approaches (more on this later).\nEvery time a prompt is updated & changes saved, a script runs converting the prompt into a python function.\nThe module holding all “prompt functions” can be loaded in to scripts where prompts are deployed.\n\nCurrently my solution will only work on linux. Other requirements:\n\ndirenv - this allows us to make a project-specific monitoring hook using an .envrc file\ninotify-tools - we use this for monitoring the prompts folder\nat - this allows our hook script (which calls inotifywait) to run from .envrc without hanging up the terminal\njinja2 python package - used for rendering prompt templates\nquarto - for a nice editing experience with syntax highlighting goodies"
  },
  {
    "objectID": "quarto-prompts.html#steps",
    "href": "quarto-prompts.html#steps",
    "title": "Quarto prompts to python functions",
    "section": "Steps",
    "text": "Steps\n\n1. Create bash hook script\nMake sure you have inotifywait available (if not, install inotify-tools). Create a hook.sh script as follows (or name it as you like).\n#!/bin/bash\n\nPROMPTS=/path/to/prompts\n\ninotifywait -m -e close_write,create,delete --format '%w%f' \"$PROMPTS\" | while read FILE\ndo\n  if [[ \"$FILE\" ==  *.qmd ]]; then\n    python prompt_loader.py --path $PROMPTS\n  fi\ndone\nThis script uses inotifywait to monitor the PROMPTS directory for changes. Whenever a .qmd file is modified, created, or deleted within this directory, it runs the prompt_loader.py script.\nMake sure to make the script executable:\nchmod +x hook.sh\n\n\n2. Setup direnv/envrc\nEnsure direnv and at are installed on your system. Create an .envrc file in the root directory of your project as follows:\npid=$(ps aux | grep -v grep | grep hook.sh | awk '{print $2}')\nif [ -z \"$pid\" ]; then\n  echo \"Initial run\"\n  python prompt_loader.py --path prompts\n  echo \"Starting hook...\"\n  echo './hook.sh' | at now\nelse\n  echo \"Hook is already running\"\nfi\nThis script checks if hook.sh is already running. If not, it runs prompt_loader.py once and schedules the hook.sh script to run immediately using the at command.\n\n\n3. Python script to write prompts to python functions\nCreate a prompt_loader.py script in the root directory of your project. Code is below or just run wget \"https://gist.github.com/jordantgh/1b22990cd66671e33a1aeef9f2cc2350\".\n\n\nCode\nimport os\nimport re\nimport sys\nimport importlib.util\nimport argparse\n\n\ndef extract_variables(template_content):\n    pattern = re.compile(r\"&lt;var\\s+(\\w+)\\s*&gt;\")\n    return sorted(set(pattern.findall(template_content)))\n\n\ndef generate_function_code(\n    function_name, template_content, variables, template_path\n):\n    params = \", \".join([f\"{var}: str = None\" for var in variables])\n    template_args = \", \".join([f\"{var}={var}\" for var in variables])\n    return f\"\"\"\ndef {function_name}({params}) -&gt; str:\n    '''/{template_path}\\n---\\n{template_content}'''\n    template = env.get_template('{os.path.basename(template_path)}')\n    return template.render({template_args})\n\"\"\"\n\n\ndef setup(prompts_dir):\n    code_string = f\"\"\"from jinja2 import Environment, FileSystemLoader\n    \nenv = Environment(\n    loader=FileSystemLoader('{prompts_dir}'),\n    variable_start_string='&lt;var',\n    variable_end_string='&gt;',\n    comment_start_string='&lt;!--',\n    comment_end_string='--&gt;',\n)\n    \"\"\"\n\n    templates = [f for f in os.listdir(prompts_dir) if f.endswith(\".qmd\")]\n\n    for filename in templates:\n        template_path = os.path.join(prompts_dir, filename)\n        function_name = filename[:-4]  # Remove .qmd extension\n        with open(template_path, \"r\") as f:\n            template_content = f.read()\n\n        variables = extract_variables(template_content)\n        code_string += generate_function_code(\n            function_name, template_content, variables, template_path\n        )\n\n    functions_dir = os.path.join(prompts_dir, \"functions\")\n    os.makedirs(functions_dir, exist_ok=True)\n\n    temp_file = os.path.join(functions_dir, \"pyprompts.py\")\n\n    with open(temp_file, \"w\") as f:\n        f.write(code_string)\n\n    pycache_path = os.path.join(functions_dir, \"__pycache__\")\n    if not os.path.exists(pycache_path) or not any(\n        f.endswith(\".pyc\") for f in os.listdir(pycache_path)\n    ):\n        spec = importlib.util.spec_from_file_location(\"pyprompts\", temp_file)\n        module = importlib.util.module_from_spec(spec)\n        sys.modules[spec.name] = module\n        spec.loader.exec_module(module)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"Provide path to prompts directory\"\n    )\n\n    parser.add_argument(\"--path\", type=str, help=\"path to prompts directory\")\n    args = parser.parse_args()\n    setup(args.path)\n\n\nThis script:\n\nExtracts Jinja2 variables from the prompt templates, using custom syntax adapted for quarto’s conventions\nGenerates Python functions for each prompt template\nCreates a pyprompts.py module in the PROMPTS directory containing all the generated functions\nImports the pyprompts module into the current Python environment (this )\n\n\n\n4. Import prompts module in your code\nTo use the generated prompt functions in your code, simply import the pyprompts module:\nimport prompts.functions.pyprompts\nNow you all of your prompts will be available with intellisense."
  }
]